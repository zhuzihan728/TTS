# TTS Model Evaluator

## Description:
This project contains a simple web interface to evaluate the performance of a TTS model on given dataset and produce the Phoneme Error Rate (PER) and Deepfake Detection Confidence (DDC) metrics.
The repository also contains several modern dockerized TTS models that can be easily built and run using the provided docker files.
## Metrics:
 - Phoneme Error Rate (PER): The PER is a metric that measures the difference between the phoneme sequence generated by the TTS model and the ground truth phoneme sequence. Since modern TTS models noramlly skip the phoneme generation step, the phoneme sequence is generated by first converting the generated speech to text using [Whisper](https://github.com/openai/whisper/tree/main) models and then converting the text to phonemes. The PER is calculated using the Levenshtein distance between the two sequences. The reason we don't use the WER metric is that the phoneme sequence is more informative in the context of TTS evaluation and it kinda mitigates the bias of the ASR model.
 - Automatic Mean Opinion Score (AMOS): We use 